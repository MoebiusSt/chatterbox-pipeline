# Chatterbox-CleanUNet

Ein vollst√§ndiges CleanUNet-basiertes System zur selektiven Entfernung von TTS-Artefakten aus Audiodateien. Das System ist darauf spezialisiert, wiederkehrende S√§usel- und Sirrger√§usche in Voice-Cloning-generierten Audiodateien zu behandeln.

## üöÄ Features

- **State-of-the-Art CleanUNet Architektur** mit Self-Attention
- **Chunk-basierte Verarbeitung** f√ºr lange Audiodateien
- **Batch-Verarbeitung** f√ºr effiziente Massenbearbeitung
- **RTX 4080 optimiert** (16GB VRAM)
- **Vollst√§ndige CLI-Integration**
- **Umfassende Evaluation-Metriken** (PESQ, STOI, SNR, etc.)
- **Mixed Precision Training** f√ºr optimale Performance
- **Reproduzierbare Ergebnisse**

## üìã Inhaltsverzeichnis

- [Installation](#installation)
- [Schnellstart](#schnellstart)
- [Training](#training)
- [Inferenz](#inferenz)
- [Konfiguration](#konfiguration)
- [Daten-Vorbereitung](#daten-vorbereitung)
- [Evaluation](#evaluation)
- [API-Referenz](#api-referenz)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

## üõ†Ô∏è Installation

### Systemanforderungen

- Python 3.8+
- CUDA 11.0+ (optional, f√ºr GPU-Beschleunigung)
- 16GB RAM (minimal), 32GB empfohlen
- 16GB VRAM (f√ºr Training auf RTX 4080)

### 1. Repository klonen

```bash
git clone <repository-url>
cd TTS-Pipeline-enhanced/Chatterbox-CleanUNet
```

### 2. Abh√§ngigkeiten installieren

```bash
# Virtuelle Umgebung erstellen (empfohlen)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate  # Windows

# Abh√§ngigkeiten installieren
pip install -r requirements.txt

# Projekt installieren
pip install -e .
```

### 3. Verzeichnisstruktur pr√ºfen

```bash
ls -la
```

Sollte folgende Struktur anzeigen:
```
Chatterbox-CleanUNet/
‚îú‚îÄ‚îÄ config/             # Konfigurationsdateien
‚îú‚îÄ‚îÄ src/               # Quellcode
‚îú‚îÄ‚îÄ scripts/           # CLI-Tools
‚îú‚îÄ‚îÄ data/              # Datenverzeichnisse
‚îú‚îÄ‚îÄ models/            # Modell-Checkpoints
‚îú‚îÄ‚îÄ outputs/           # Ausgaben und Logs
‚îî‚îÄ‚îÄ tests/             # Tests
```

## ‚ö° Schnellstart

### Audio enhancen (mit vortrainiertem Modell)

```bash
# Einzelne Datei
python scripts/enhance_audio.py input.wav output.wav --model models/pretrained/cleanunet_best.pth

# Batch-Verarbeitung
python scripts/enhance_audio.py input_dir/ output_dir/ --batch --verbose

# Mit CPU (falls keine GPU verf√ºgbar)
python scripts/enhance_audio.py input.wav output.wav --cpu
```

### Training starten

```bash
# Standard-Training
python scripts/train.py

# Mit eigenen Daten
python scripts/train.py --train_data data/my_train --val_data data/my_val

# Training fortsetzen
python scripts/train.py --resume models/checkpoints/checkpoint_epoch_50.pth
```

## üéØ Training

### Kompletter Training-Workflow

Der Training-Prozess besteht aus **4 aufeinanderfolgenden Schritten**:

```mermaid
graph LR
    A[1. Datenvorbereitung] --> B[2. Training]
    B --> C[3. Evaluation]
    C --> D[4. Produktive Nutzung]
```

#### **Warum 3 Datensets (train/validation/test)?**

```
Deine Audio-Paare (100%)
‚îú‚îÄ‚îÄ train/ (80%)      ‚Üí Modell lernt von diesen Daten
‚îú‚îÄ‚îÄ validation/ (10%) ‚Üí √úberwacht Lernfortschritt, verhindert Overfitting
‚îî‚îÄ‚îÄ test/ (10%)       ‚Üí Unabh√§ngige finale Qualit√§tsbewertung
```

### 1. Schritt: Daten vorbereiten

#### **üéØ Du brauchst nur EINEN Datensatz!**

Sammle **alle** deine Audio-Paare in zwei Ordnern:

```
your_audio_collection/
‚îú‚îÄ‚îÄ clean/                    # ALLE deine sauberen Aufnahmen
‚îÇ   ‚îú‚îÄ‚îÄ speaker1_001.wav
‚îÇ   ‚îú‚îÄ‚îÄ speaker1_002.wav
‚îÇ   ‚îú‚îÄ‚îÄ speaker2_001.wav
‚îÇ   ‚îî‚îÄ‚îÄ ... (z.B. 1000 Dateien)
‚îî‚îÄ‚îÄ noisy/                    # Entsprechende verrauschte Versionen  
    ‚îú‚îÄ‚îÄ speaker1_001.wav      # Gleiche Dateinamen!
    ‚îú‚îÄ‚îÄ speaker1_002.wav
    ‚îú‚îÄ‚îÄ speaker2_001.wav
    ‚îî‚îÄ‚îÄ ... (1000 entsprechende Dateien)
```

**Option A: Automatische Aufteilung (empfohlen)**
```bash
# Automatische Aufteilung: 80% train, 10% validation, 10% test
python scripts/prepare_dataset.py \
    --clean_dir your_audio_collection/clean \
    --noisy_dir your_audio_collection/noisy \
    --output_dir data/processed
```

**‚ûú Das Script verteilt ZUF√ÑLLIG deine Audio-Paare auf train/validation/test!**

**Option B: Manuelle Struktur erstellen**
```
data/processed/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ clean/          # 80% deiner sauberen Audio-Dateien
‚îÇ   ‚îî‚îÄ‚îÄ noisy/          # 80% der entsprechenden verrauschten Dateien
‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îú‚îÄ‚îÄ clean/          # 10% f√ºr Validierung w√§hrend Training
‚îÇ   ‚îî‚îÄ‚îÄ noisy/          # 10% entsprechende verrauschte Dateien
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ clean/          # 10% f√ºr finale Evaluation
    ‚îî‚îÄ‚îÄ noisy/          # 10% entsprechende verrauschte Dateien
```

**Wichtig:** Die Dateinamen in `clean/` und `noisy/` m√ºssen √ºbereinstimmen!

#### **üé≤ Beispiel: Automatische Aufteilung**

Du hast **1000 Audio-Paare** gesammelt:

**Deine Eingabe:**
```
your_audio_collection/clean/    ‚Üí 1000 saubere .wav Dateien
your_audio_collection/noisy/    ‚Üí 1000 entsprechende verrauschte .wav Dateien
```

**prepare_dataset.py erstellt automatisch:**
```
data/processed/
‚îú‚îÄ‚îÄ train/              # 800 zuf√§llige Paare (80%)
‚îÇ   ‚îú‚îÄ‚îÄ clean/
‚îÇ   ‚îî‚îÄ‚îÄ noisy/
‚îú‚îÄ‚îÄ validation/         # 100 zuf√§llige Paare (10%)
‚îÇ   ‚îú‚îÄ‚îÄ clean/
‚îÇ   ‚îî‚îÄ‚îÄ noisy/
‚îî‚îÄ‚îÄ test/              # 100 zuf√§llige Paare (10%)
    ‚îú‚îÄ‚îÄ clean/
    ‚îî‚îÄ‚îÄ noisy/
```

**Du musst NICHT selbst entscheiden welche Dateien wohin kommen!**

### 2. Schritt: Training durchf√ºhren

**2.1 Konfiguration anpassen (optional)**

Bearbeite `config/train_config.yaml` und `config/model_config.yaml` nach deinen Bed√ºrfnissen:

```yaml
# config/train_config.yaml
training:
  batch_size: 8          # Reduziere bei GPU-Speicherproblemen
  num_epochs: 100
  learning_rate: 0.0001
  
hardware:
  device: "cuda"
  mixed_precision: true  # F√ºr RTX 4080 empfohlen
```

**2.2 Training starten**

```bash
# Standard-Training (verwendet train/ und validation/ automatisch)
python scripts/train.py

# Mit eigenen Pfaden
python scripts/train.py \
    --train_data data/processed/train \
    --val_data data/processed/validation \
    --output_dir outputs/my_training
```

**2.3 Training √ºberwachen**

```bash
# Tensorboard starten (parallel zum Training)
tensorboard --logdir outputs/training/logs
```
‚Üí √ñffne http://localhost:6006 im Browser f√ºr Live-Monitoring

**Was passiert w√§hrend dem Training?**
- Modell lernt von `train/`-Daten
- Validiert sich selbst an `validation/`-Daten
- Speichert beste Checkpoints automatisch
- Stoppt bei Overfitting (Early Stopping)

### 3. Schritt: Modell evaluieren

Nach dem Training ‚Üí Finale Qualit√§tsbewertung:

```bash
# Evaluation auf unabh√§ngigem test/-Set
python scripts/evaluate_model.py \
    --model models/final/cleanunet_best.pth \
    --test_data data/processed/test \
    --output_dir outputs/evaluation
```

**Was macht evaluate_model.py?**
- Testet das trainierte Modell auf `test/`-Daten (die es noch nie gesehen hat)
- Berechnet objektive Metriken (PESQ, STOI, SNR)
- Erstellt detaillierten Qualit√§ts-Report
- Speichert Enhanced Audio-Beispiele

### 4. Schritt: Produktive Nutzung

Nach erfolgreichem Training ‚Üí Echte Audio-Dateien enhancen:

```bash
# Neue Audio-Dateien verarbeiten
python scripts/enhance_audio.py input_audio.wav enhanced_audio.wav \
    --model models/final/cleanunet_best.pth

# Batch-Verarbeitung f√ºr viele Dateien
python scripts/enhance_audio.py input_directory/ output_directory/ \
    --batch --verbose
```

## üìã **Training-Checkliste**

```
‚òê 1. Audio-Paare gesammelt (clean + noisy, gleiche Dateinamen)
‚òê 2. prepare_dataset.py ausgef√ºhrt ‚Üí automatische train/val/test Aufteilung
‚òê 3. train.py gestartet ‚Üí Modell trainiert mit train/, validiert mit validation/
‚òê 4. evaluate_model.py ausgef√ºhrt ‚Üí finale Qualit√§tsbewertung mit test/
‚òê 5. enhance_audio.py getestet ‚Üí produktiv einsatzbereit
```

## ‚ùì **H√§ufige Fragen (FAQ)**

**Q: Muss ich separate Datens√§tze f√ºr Training und Validation erstellen?**
**A: NEIN!** Du sammelst alle Audio-Paare in einem Ordner. `prepare_dataset.py` teilt automatisch auf.

**Q: Woher wei√ü das Script, welche Dateien zu train/validation/test geh√∂ren?**
**A: Zuf√§llige Aufteilung!** Das Script mischt alle Paare und teilt sie prozentual auf (80/10/10).

**Q: K√∂nnen sich train/validation/test-Daten √ºberschneiden?**
**A: NEIN!** Jedes Audio-Paar kommt nur in EIN Set. Das garantiert unabh√§ngige Evaluation.

**Q: Was ist der Unterschied zwischen validation/ und test/?**
**A:** 
- **validation/**: Wird W√ÑHREND dem Training f√ºr Early Stopping verwendet
- **test/**: Wird NACH dem Training f√ºr finale, unabh√§ngige Qualit√§tsbewertung verwendet

## üîÑ **Script-√úbersicht**

| Script | Zweck | Wann verwenden |
|--------|-------|----------------|
| `prepare_dataset.py` | Daten aufteilen | **Einmalig** vor Training |
| `train.py` | Modell trainieren | **Einmalig** f√ºr jedes Modell |
| `evaluate_model.py` | Qualit√§t bewerten | **Nach** jedem Training |
| `enhance_audio.py` | Audio verbessern | **Produktiv** f√ºr echte Dateien |

## üîÆ Inferenz

### Einzelne Datei verarbeiten

```bash
python scripts/enhance_audio.py input.wav output.wav \
    --model models/final/cleanunet_best.pth \
    --verbose
```

### Batch-Verarbeitung

```bash
# Alle WAV-Dateien in einem Verzeichnis
python scripts/enhance_audio.py input_dir/ output_dir/ \
    --batch \
    --pattern "*.wav" \
    --verbose

# Rekursive Verarbeitung
python scripts/enhance_audio.py input_dir/ output_dir/ \
    --batch \
    --recursive \
    --overwrite
```

### Mit Quality-Evaluation

```bash
python scripts/enhance_audio.py noisy.wav enhanced.wav \
    --reference clean.wav \
    --metrics
```

### Erweiterte Optionen

```bash
python scripts/enhance_audio.py input.wav output.wav \
    --model models/my_model.pth \
    --chunk_size 65536 \
    --sample_rate 22050 \
    --device cuda \
    --no_normalize \
    --suffix "_denoised"
```

## ‚öôÔ∏è Konfiguration

### Model-Konfiguration (`config/model_config.yaml`)

```yaml
model:
  # Audio-Parameter
  sample_rate: 24000
  n_fft: 1024
  hop_length: 256
  
  # Modell-Architektur
  encoder_channels: [64, 128, 256, 512, 512, 512, 512, 512]
  decoder_channels: [512, 512, 512, 512, 256, 128, 64, 32]
  
  # Self-Attention
  attention_heads: 8
  attention_dim: 512
  attention_layers: 4
```

### Training-Konfiguration (`config/train_config.yaml`)

```yaml
training:
  batch_size: 8
  num_epochs: 100
  learning_rate: 0.0001
  
  # Scheduler
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 10
  
  # Early Stopping
  early_stopping_patience: 20
  
hardware:
  device: "cuda"
  mixed_precision: true
```

### Inferenz-Konfiguration (`config/inference_config.yaml`)

```yaml
inference:
  model_path: "models/final/cleanunet_best.pth"
  device: "cuda"
  chunk_size: 48000    # 2 Sekunden bei 24kHz
  overlap: 0.25        # 25% √úberlappung
```

## üìä Daten-Vorbereitung

### Automatische Datensatz-Vorbereitung

```bash
python scripts/prepare_dataset.py \
    --clean_dir path/to/clean/audio \
    --noisy_dir path/to/noisy/audio \
    --output_dir data/processed \
    --train_ratio 0.8 \
    --val_ratio 0.1 \
    --test_ratio 0.1
```

### Manuelle Vorbereitung

1. **Clean Audio**: Hochqualitative, saubere Referenz-Aufnahmen
2. **Noisy Audio**: Entsprechende Versionen mit TTS-Artefakten
3. **Naming**: Identische Dateinamen f√ºr Paare
4. **Format**: WAV, 16kHz, Mono empfohlen

**Beispiel:**
```
train/clean/speaker1_001.wav    ‚Üê‚Üí    train/noisy/speaker1_001.wav
train/clean/speaker1_002.wav    ‚Üê‚Üí    train/noisy/speaker1_002.wav
```

### Datenanforderungen

- **Minimum**: 100 Stunden Audio-Paare
- **Empfohlen**: 500+ Stunden
- **Format**: WAV, 24kHz (optimiert)
- **Qualit√§t**: Clean audio SNR > 40dB

## üìà Evaluation

### Modell evaluieren

```bash
python scripts/evaluate_model.py \
    --model models/final/cleanunet_best.pth \
    --test_data data/processed/test \
    --output_dir outputs/evaluation
```

### Metriken verstehen

- **PESQ**: Perceptual Evaluation (1.0-4.5, h√∂her = besser)
- **STOI**: Short-Time Objective Intelligibility (0-1, h√∂her = besser)
- **SI-SNR**: Scale-Invariant SNR in dB (h√∂her = besser)
- **LSD**: Log-Spectral Distance (niedriger = besser)

### Benchmark-Ergebnisse

| Model | PESQ | STOI | SI-SNR | Real-time Factor |
|-------|------|------|--------|------------------|
| Baseline | 1.8 | 0.75 | 8.5 dB | - |
| CleanUNet | 3.2 | 0.92 | 18.2 dB | 0.15x |

## üîß API-Referenz

### Python API

```python
from src.inference.enhancer import AudioEnhancer
from src.utils.audio_utils import load_audio, save_audio

# Enhancer erstellen
enhancer = AudioEnhancer(inference_config, model_config)

# Audio laden
audio, sr = load_audio("input.wav", sample_rate=24000)

# Audio enhancen
enhanced = enhancer.enhance_audio(audio)

# Speichern
save_audio(enhanced, "output.wav", sample_rate=24000)
```

### Training API

```python
from src.training.trainer import Trainer
from src.models.cleanunet import CleanUNet
from src.models.loss import CleanUNetLoss

# Modell erstellen
model = CleanUNet(model_config)
criterion = CleanUNetLoss(loss_config)

# Trainer erstellen
trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, config, device)

# Training starten
trainer.train()
```

## üö® Troubleshooting

### H√§ufige Probleme

#### 1. CUDA Out of Memory

```bash
# Batch-Size reduzieren
python scripts/train.py --config config/train_config_small.yaml

# Chunk-Size reduzieren
python scripts/enhance_audio.py input.wav output.wav --chunk_size 16384
```

#### 2. Langsames Training

```bash
# Mixed Precision aktivieren
python scripts/train.py --mixed_precision

# Mehr Worker
# In config/train_config.yaml: num_workers: 8
```

#### 3. Schlechte Qualit√§t

- Mehr Trainingsdaten verwenden
- L√§ngeres Training (mehr Epochen)
- Learning Rate anpassen
- Datenqualit√§t pr√ºfen

#### 4. Import-Fehler

```bash
# PYTHONPATH setzen
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Oder Projekt neu installieren
pip install -e .
```

### Debug-Modus

```bash
# Training debuggen
python scripts/train.py --debug

# Inferenz debuggen
python scripts/enhance_audio.py input.wav output.wav --verbose
```

### Log-Dateien

```bash
# Training-Logs
tail -f outputs/training/logs/tensorboard.log

# Fehler-Logs
grep -i error outputs/training/logs/*.log
```

## üß™ Tests

```bash
# Alle Tests ausf√ºhren
python -m pytest tests/

# Spezifische Tests
python -m pytest tests/test_model.py
python -m pytest tests/test_inference.py

# Mit Coverage
python -m pytest tests/ --cov=src/
```

## üìö Weiterf√ºhrende Ressourcen

- [CleanUNet Paper](https://arxiv.org/abs/2202.09047)
- [Audio Enhancement Best Practices](docs/best_practices.md)
- [Model Architecture Details](docs/architecture.md)
- [Performance Optimization](docs/optimization.md)

## ü§ù Contributing

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/amazing-feature`)
3. Committe deine √Ñnderungen (`git commit -m 'Add amazing feature'`)
4. Pushe zum Branch (`git push origin feature/amazing-feature`)
5. √ñffne eine Pull Request

## üìù Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert. Siehe [LICENSE](LICENSE) f√ºr Details.

## üôè Acknowledgments

- NVIDIA f√ºr die urspr√ºngliche CleanUNet-Implementierung
- PyTorch Team f√ºr das Framework
- Audio-Community f√ºr Evaluation-Metriken

## üìû Support

Bei Fragen oder Problemen:

1. Pr√ºfe die [FAQ](docs/faq.md)
2. Durchsuche die [Issues](issues)
3. Erstelle ein neues Issue mit detaillierter Beschreibung

---

**Happy Denoising! üéµ‚ú®** 