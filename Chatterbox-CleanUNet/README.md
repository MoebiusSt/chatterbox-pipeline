# Chatterbox-CleanUNet

Ein vollst√§ndiges CleanUNet-basiertes System zur selektiven Entfernung von TTS-Artefakten aus Audiodateien. Das System ist darauf spezialisiert, wiederkehrende S√§usel- und Sirrger√§usche in Voice-Cloning-generierten Audiodateien zu behandeln.

## üöÄ Features

- **State-of-the-Art CleanUNet Architektur** mit Self-Attention
- **Chunk-basierte Verarbeitung** f√ºr lange Audiodateien
- **Batch-Verarbeitung** f√ºr effiziente Massenbearbeitung
- **RTX 4080 optimiert** (16GB VRAM)
- **Vollst√§ndige CLI-Integration**
- **Umfassende Evaluation-Metriken** (PESQ, STOI, SNR, etc.)
- **Mixed Precision Training** f√ºr optimale Performance
- **Reproduzierbare Ergebnisse**

## üìã Inhaltsverzeichnis

- [Installation](#installation)
- [Schnellstart](#schnellstart)
- [Training](#training)
- [Inferenz](#inferenz)
- [Konfiguration](#konfiguration)
- [Daten-Vorbereitung](#daten-vorbereitung)
- [Evaluation](#evaluation)
- [API-Referenz](#api-referenz)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

## üõ†Ô∏è Installation

### Systemanforderungen

- Python 3.8+
- CUDA 11.0+ (optional, f√ºr GPU-Beschleunigung)
- 16GB RAM (minimal), 32GB empfohlen
- 16GB VRAM (f√ºr Training auf RTX 4080)

### 1. Repository klonen

```bash
git clone <repository-url>
cd TTS-Pipeline-enhanced/Chatterbox-CleanUNet
```

### 2. Abh√§ngigkeiten installieren

```bash
# Virtuelle Umgebung erstellen (empfohlen)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate  # Windows

# Abh√§ngigkeiten installieren
pip install -r requirements.txt

# Projekt installieren
pip install -e .
```

### 3. Verzeichnisstruktur pr√ºfen

```bash
ls -la
```

Sollte folgende Struktur anzeigen:
```
Chatterbox-CleanUNet/
‚îú‚îÄ‚îÄ config/             # Konfigurationsdateien
‚îú‚îÄ‚îÄ src/               # Quellcode
‚îú‚îÄ‚îÄ scripts/           # CLI-Tools
‚îú‚îÄ‚îÄ data/              # Datenverzeichnisse
‚îú‚îÄ‚îÄ models/            # Modell-Checkpoints
‚îú‚îÄ‚îÄ outputs/           # Ausgaben und Logs
‚îî‚îÄ‚îÄ tests/             # Tests
```

## ‚ö° Schnellstart

### Audio enhancen (mit vortrainiertem Modell)

```bash
# Einzelne Datei
python scripts/enhance_audio.py input.wav output.wav --model models/pretrained/cleanunet_best.pth

# Batch-Verarbeitung
python scripts/enhance_audio.py input_dir/ output_dir/ --batch --verbose

# Mit CPU (falls keine GPU verf√ºgbar)
python scripts/enhance_audio.py input.wav output.wav --cpu
```

### Training starten

```bash
# Standard-Training
python scripts/train.py

# Mit eigenen Daten
python scripts/train.py --train_data data/my_train --val_data data/my_val

# Training fortsetzen
python scripts/train.py --resume models/checkpoints/checkpoint_epoch_50.pth
```

## üéØ Training

### 1. Daten vorbereiten

Erstelle deine Trainings-/Validierungsdaten in folgender Struktur:

```
data/processed/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ clean/          # Saubere Reference-Audiodateien
‚îÇ   ‚îî‚îÄ‚îÄ noisy/          # Verrauschte/artifacted Audiodateien
‚îî‚îÄ‚îÄ validation/
    ‚îú‚îÄ‚îÄ clean/          # Saubere Reference-Audiodateien
    ‚îî‚îÄ‚îÄ noisy/          # Verrauschte/artifacted Audiodateien
```

**Wichtig:** Die Dateinamen in `clean/` und `noisy/` m√ºssen √ºbereinstimmen!

### 2. Konfiguration anpassen

Bearbeite `config/train_config.yaml` und `config/model_config.yaml` nach deinen Bed√ºrfnissen:

```yaml
# config/train_config.yaml
training:
  batch_size: 8          # Reduziere bei Speicherproblemen
  num_epochs: 100
  learning_rate: 0.0001
  
hardware:
  device: "cuda"
  mixed_precision: true  # F√ºr RTX 4080 empfohlen
```

### 3. Training starten

```bash
# Basis-Training
python scripts/train.py

# Mit erweiterten Optionen
python scripts/train.py \
    --config config/train_config.yaml \
    --model_config config/model_config.yaml \
    --output_dir outputs/my_training \
    --mixed_precision \
    --verbose
```

### 4. Training √ºberwachen

```bash
# Tensorboard starten
tensorboard --logdir outputs/training/logs
```

√ñffne http://localhost:6006 im Browser.

### 5. Training-Fortschritt pr√ºfen

```bash
# Logs anzeigen
tail -f outputs/training/logs/training.log

# Checkpoints auflisten
ls -la models/checkpoints/
```

## üîÆ Inferenz

### Einzelne Datei verarbeiten

```bash
python scripts/enhance_audio.py input.wav output.wav \
    --model models/final/cleanunet_best.pth \
    --verbose
```

### Batch-Verarbeitung

```bash
# Alle WAV-Dateien in einem Verzeichnis
python scripts/enhance_audio.py input_dir/ output_dir/ \
    --batch \
    --pattern "*.wav" \
    --verbose

# Rekursive Verarbeitung
python scripts/enhance_audio.py input_dir/ output_dir/ \
    --batch \
    --recursive \
    --overwrite
```

### Mit Quality-Evaluation

```bash
python scripts/enhance_audio.py noisy.wav enhanced.wav \
    --reference clean.wav \
    --metrics
```

### Erweiterte Optionen

```bash
python scripts/enhance_audio.py input.wav output.wav \
    --model models/my_model.pth \
    --chunk_size 65536 \
    --sample_rate 22050 \
    --device cuda \
    --no_normalize \
    --suffix "_denoised"
```

## ‚öôÔ∏è Konfiguration

### Model-Konfiguration (`config/model_config.yaml`)

```yaml
model:
  # Audio-Parameter
  sample_rate: 16000
  n_fft: 512
  hop_length: 128
  
  # Modell-Architektur
  encoder_channels: [64, 128, 256, 512, 512, 512, 512, 512]
  decoder_channels: [512, 512, 512, 512, 256, 128, 64, 32]
  
  # Self-Attention
  attention_heads: 8
  attention_dim: 512
  attention_layers: 4
```

### Training-Konfiguration (`config/train_config.yaml`)

```yaml
training:
  batch_size: 8
  num_epochs: 100
  learning_rate: 0.0001
  
  # Scheduler
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 10
  
  # Early Stopping
  early_stopping_patience: 20
  
hardware:
  device: "cuda"
  mixed_precision: true
```

### Inferenz-Konfiguration (`config/inference_config.yaml`)

```yaml
inference:
  model_path: "models/final/cleanunet_best.pth"
  device: "cuda"
  chunk_size: 32768    # 2 Sekunden bei 16kHz
  overlap: 0.25        # 25% √úberlappung
```

## üìä Daten-Vorbereitung

### Automatische Datensatz-Vorbereitung

```bash
python scripts/prepare_dataset.py \
    --clean_dir path/to/clean/audio \
    --noisy_dir path/to/noisy/audio \
    --output_dir data/processed \
    --train_ratio 0.8 \
    --val_ratio 0.1 \
    --test_ratio 0.1
```

### Manuelle Vorbereitung

1. **Clean Audio**: Hochqualitative, saubere Referenz-Aufnahmen
2. **Noisy Audio**: Entsprechende Versionen mit TTS-Artefakten
3. **Naming**: Identische Dateinamen f√ºr Paare
4. **Format**: WAV, 16kHz, Mono empfohlen

**Beispiel:**
```
train/clean/speaker1_001.wav    ‚Üê‚Üí    train/noisy/speaker1_001.wav
train/clean/speaker1_002.wav    ‚Üê‚Üí    train/noisy/speaker1_002.wav
```

### Datenanforderungen

- **Minimum**: 100 Stunden Audio-Paare
- **Empfohlen**: 500+ Stunden
- **Format**: WAV, 16-48kHz
- **Qualit√§t**: Clean audio SNR > 40dB

## üìà Evaluation

### Modell evaluieren

```bash
python scripts/evaluate_model.py \
    --model models/final/cleanunet_best.pth \
    --test_data data/processed/test \
    --output_dir outputs/evaluation
```

### Metriken verstehen

- **PESQ**: Perceptual Evaluation (1.0-4.5, h√∂her = besser)
- **STOI**: Short-Time Objective Intelligibility (0-1, h√∂her = besser)
- **SI-SNR**: Scale-Invariant SNR in dB (h√∂her = besser)
- **LSD**: Log-Spectral Distance (niedriger = besser)

### Benchmark-Ergebnisse

| Model | PESQ | STOI | SI-SNR | Real-time Factor |
|-------|------|------|--------|------------------|
| Baseline | 1.8 | 0.75 | 8.5 dB | - |
| CleanUNet | 3.2 | 0.92 | 18.2 dB | 0.15x |

## üîß API-Referenz

### Python API

```python
from src.inference.enhancer import AudioEnhancer
from src.utils.audio_utils import load_audio, save_audio

# Enhancer erstellen
enhancer = AudioEnhancer(inference_config, model_config)

# Audio laden
audio, sr = load_audio("input.wav", sample_rate=16000)

# Audio enhancen
enhanced = enhancer.enhance_audio(audio)

# Speichern
save_audio(enhanced, "output.wav", sample_rate=16000)
```

### Training API

```python
from src.training.trainer import Trainer
from src.models.cleanunet import CleanUNet
from src.models.loss import CleanUNetLoss

# Modell erstellen
model = CleanUNet(model_config)
criterion = CleanUNetLoss(loss_config)

# Trainer erstellen
trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, config, device)

# Training starten
trainer.train()
```

## üö® Troubleshooting

### H√§ufige Probleme

#### 1. CUDA Out of Memory

```bash
# Batch-Size reduzieren
python scripts/train.py --config config/train_config_small.yaml

# Chunk-Size reduzieren
python scripts/enhance_audio.py input.wav output.wav --chunk_size 16384
```

#### 2. Langsames Training

```bash
# Mixed Precision aktivieren
python scripts/train.py --mixed_precision

# Mehr Worker
# In config/train_config.yaml: num_workers: 8
```

#### 3. Schlechte Qualit√§t

- Mehr Trainingsdaten verwenden
- L√§ngeres Training (mehr Epochen)
- Learning Rate anpassen
- Datenqualit√§t pr√ºfen

#### 4. Import-Fehler

```bash
# PYTHONPATH setzen
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Oder Projekt neu installieren
pip install -e .
```

### Debug-Modus

```bash
# Training debuggen
python scripts/train.py --debug

# Inferenz debuggen
python scripts/enhance_audio.py input.wav output.wav --verbose
```

### Log-Dateien

```bash
# Training-Logs
tail -f outputs/training/logs/tensorboard.log

# Fehler-Logs
grep -i error outputs/training/logs/*.log
```

## üß™ Tests

```bash
# Alle Tests ausf√ºhren
python -m pytest tests/

# Spezifische Tests
python -m pytest tests/test_model.py
python -m pytest tests/test_inference.py

# Mit Coverage
python -m pytest tests/ --cov=src/
```

## üìö Weiterf√ºhrende Ressourcen

- [CleanUNet Paper](https://arxiv.org/abs/2202.09047)
- [Audio Enhancement Best Practices](docs/best_practices.md)
- [Model Architecture Details](docs/architecture.md)
- [Performance Optimization](docs/optimization.md)

## ü§ù Contributing

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/amazing-feature`)
3. Committe deine √Ñnderungen (`git commit -m 'Add amazing feature'`)
4. Pushe zum Branch (`git push origin feature/amazing-feature`)
5. √ñffne eine Pull Request

## üìù Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert. Siehe [LICENSE](LICENSE) f√ºr Details.

## üôè Acknowledgments

- NVIDIA f√ºr die urspr√ºngliche CleanUNet-Implementierung
- PyTorch Team f√ºr das Framework
- Audio-Community f√ºr Evaluation-Metriken

## üìû Support

Bei Fragen oder Problemen:

1. Pr√ºfe die [FAQ](docs/faq.md)
2. Durchsuche die [Issues](issues)
3. Erstelle ein neues Issue mit detaillierter Beschreibung

---

**Happy Denoising! üéµ‚ú®** 